import pandas as pd
import numpy as np
import argparse
import random
from tqdm.notebook import tqdm
from copy import deepcopy

import torch
import torch.nn as nn

from transformers import AutoTokenizer
from mamba_ssm.models.mixer_seq_simple import MambaLMHeadModel

# --------------------------------------------------------------------------------------------------------------------------------------------------

def delete_layers(model, layer_indices):
    """
    æ ¹æ®ç»™å®šçš„å±‚ç´¢å¼•åˆ—è¡¨æ‰¹é‡åˆ é™¤ Mamba æ¨¡å‹å±‚
    :param model: åŸå§‹ Mamba æ¨¡å‹
    :param layer_indices: è¦åˆ é™¤çš„å±‚ç´¢å¼•åˆ—è¡¨ï¼ˆ0-basedï¼‰
    """
    max_idx = len(model.backbone.layers) - 1
    for idx in layer_indices:
        if idx < 0 or idx > max_idx:
            raise ValueError(f"å±‚ç´¢å¼• {idx} æ— æ•ˆï¼Œå…è®¸èŒƒå›´ 0~{max_idx}")

    # é€†åºåˆ é™¤ï¼Œé¿å…ç´¢å¼•é”™ä½
    for idx in sorted(layer_indices, reverse=True):
        del model.backbone.layers[idx]

    # æ›´æ–° config
    model.config.n_layer = len(model.backbone.layers)

    return model


def mamba_prune(args):
    model_name = args.model_path
    print(f"ğŸ”¹ åŠ è½½æ¨¡å‹ {model_name} ...")

    tokenizer = AutoTokenizer.from_pretrained("gpt-neox-20b")
    model = MambaLMHeadModel.from_pretrained(model_name, device=args.device, dtype=torch.float16)
    print(model)

    layers_to_delete = [63, 61, 60, 62, 59, 57, 58, 53, 54, 56, 52, 50, 51, 0, 55, 44, 47, 46]

    print(f"ğŸ”¹ åŸå§‹å±‚æ•°: {len(model.backbone.layers)}")
    pruned_model = delete_layers(model, layers_to_delete)
    print(f"âœ… å‰ªæåå±‚æ•°: {len(pruned_model.backbone.layers)}")
    
    tokenizer.save_pretrained(f"pruned_models/mamba_layers{len(layers_to_delete)}")
    pruned_model.save_pretrained(f"pruned_models/mamba_layers{len(layers_to_delete)}")


    print(f"âœ… æ¨¡å‹å·²ä¿å­˜åˆ° {save_dir}, å‰ªæ‰äº† {len(layers_to_delete)} å±‚")


# --------------------------------------------------------------------------------------------------------------------------------------------------

if __name__ == "__main__":
    parser = argparse.ArgumentParser(description="Mamba å±‚å‰ªæ")
    parser.add_argument("--model_path", type=str, default="mamba-2.8b")
    parser.add_argument("--dataset", type=str, default="bookcorpus")
    parser.add_argument("--threshold", type=float, default=0.72)
    parser.add_argument("--high_lay", type=int)
    parser.add_argument("--target_count", type=int, default=10)
    parser.add_argument("--batch_size", type=int, default=1)
    parser.add_argument("--seed", type=int, default=1234)
    parser.add_argument("--gpu", type=int, default=0)
    args = parser.parse_args()

    # å›ºå®šéšæœºç§å­
    seed = args.seed
    random.seed(seed)
    np.random.seed(seed)
    torch.manual_seed(seed)
    if torch.cuda.is_available():
        torch.cuda.manual_seed_all(seed)
    torch.backends.cudnn.deterministic = True
    torch.backends.cudnn.benchmark = False

    # è®¾ç½®è®¾å¤‡
    if args.gpu is not None:
        args.device = torch.device(f'cuda:{args.gpu}' if torch.cuda.is_available() else 'cpu')
    else:
        args.device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')

    mamba_prune(args)
