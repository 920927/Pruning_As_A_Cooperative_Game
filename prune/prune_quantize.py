import pandas as pd
import numpy as np
import argparse
import random
from tqdm.notebook import tqdm
from copy import deepcopy

import torch
import torch.nn as nn

from transformers import AutoTokenizer
from auto_gptq import AutoGPTQForCausalLM

# --------------------------------------------------------------------------------------------------------------------------------------------------

def delete_layers(model, layer_indices):
    max_idx = len(model.model.model.layers) - 1
    for idx in layer_indices:
        if idx < 0 or idx > max_idx:
            raise ValueError(f"å±‚ç´¢å¼• {idx} æ— æ•ˆï¼Œå…è®¸èŒƒå›´ 0~{max_idx}")

    # é€†åºåˆ é™¤ï¼Œé¿å…ç´¢å¼•é”™ä½
    for idx in sorted(layer_indices, reverse=True):
        del model.model.model.layers[idx]

    # æ›´æ–° config
    model.config.num_hidden_layers = len(model.model.model.layers)

    return model


def quantize_prune(args):
    model_name = args.model_path
    print(f"ğŸ”¹ åŠ è½½æ¨¡å‹ {model_name} ...")

    model = AutoGPTQForCausalLM.from_quantized(
        model_name,
        use_safetensors=True,
        trust_remote_code=True,
        use_triton=False,
        quantize_config=None,
    )
    tokenizer = AutoTokenizer.from_pretrained(model_name)
    print(model)

    layers_to_delete = [21, 11, 12, 25, 23, 10]

    print(f"ğŸ”¹ åŸå§‹å±‚æ•°: {len(model.model.model.layers)}")
    pruned_model = delete_layers(model, layers_to_delete)
    print(f"âœ… å‰ªæåå±‚æ•°: {len(pruned_model.model.model.layers)}")
    

    pruned_model.save_pretrained(f"pruned_models/gptq_{'_'.join(map(str, layers_to_delete))}")
    tokenizer.save_pretrained(f"pruned_models/gptq_{'_'.join(map(str, layers_to_delete))}")

    print(f"âœ… æ¨¡å‹å·²ä¿å­˜åˆ° {save_dir}, å‰ªæ‰äº† {len(layers_to_delete)} å±‚")


# --------------------------------------------------------------------------------------------------------------------------------------------------

if __name__ == "__main__":
    parser = argparse.ArgumentParser(description="Mamba å±‚å‰ªæ")
    parser.add_argument("--model_path", type=str, default="GPTQ/Llama-2-7b-hf")
    parser.add_argument("--dataset", type=str, default="bookcorpus")
    parser.add_argument("--threshold", type=float, default=0.72)
    parser.add_argument("--high_lay", type=int)
    parser.add_argument("--target_count", type=int, default=10)
    parser.add_argument("--batch_size", type=int, default=1)
    parser.add_argument("--seed", type=int, default=1234)
    parser.add_argument("--gpu", type=int, default=0)
    args = parser.parse_args()

    # å›ºå®šéšæœºç§å­
    seed = args.seed
    random.seed(seed)
    np.random.seed(seed)
    torch.manual_seed(seed)
    if torch.cuda.is_available():
        torch.cuda.manual_seed_all(seed)
    torch.backends.cudnn.deterministic = True
    torch.backends.cudnn.benchmark = False

    # è®¾ç½®è®¾å¤‡
    if args.gpu is not None:
        args.device = torch.device(f'cuda:{args.gpu}' if torch.cuda.is_available() else 'cpu')
    else:
        args.device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')

    quantize_prune(args)
